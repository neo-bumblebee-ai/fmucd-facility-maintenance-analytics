{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bcda03-62ef-4e04-b0fc-85590a461df1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix CAST_INVALID_INPUT for WOStartDate and WOEndDate columns"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "CATALOG = \"fmucd_capstone\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.bronze.bronze_fmucd_raw\"\n",
    "\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "DIM_BUILDING = f\"{CATALOG}.{SILVER_SCHEMA}.dim_building_scd2\"\n",
    "DIM_ASSET = f\"{CATALOG}.{SILVER_SCHEMA}.dim_asset_scd2\"\n",
    "FACT_WO = f\"{CATALOG}.{SILVER_SCHEMA}.fact_work_orders\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SILVER_SCHEMA}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Read Bronze\n",
    "# ------------------------------------------------------------\n",
    "b = spark.table(BRONZE_TABLE)\n",
    "\n",
    "# Helper for consistent timestamps\n",
    "load_ts = F.current_timestamp()\n",
    "load_dt = F.current_date()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Building Dimension (SCD2)\n",
    "# ------------------------------------------------------------\n",
    "src_building = (\n",
    "    b.select(\n",
    "        F.col(\"BuildingID\").alias(\"building_id\"),\n",
    "        F.col(\"UniversityID\").alias(\"university_id\"),\n",
    "        F.col(\"Country\").alias(\"country\"),\n",
    "        F.col(\"State_Province\").alias(\"state_province\"),\n",
    "        F.col(\"BuildingName\").alias(\"building_name\"),\n",
    "        F.col(\"Size\").cast(\"double\").alias(\"building_size\"),\n",
    "        F.col(\"Type\").alias(\"building_type\"),\n",
    "        F.floor(F.col(\"BuiltYear\").cast(\"double\")).cast(\"int\").alias(\"built_year\"),\n",
    "        F.col(\"FCI\").cast(\"double\").alias(\"fci\"),\n",
    "        F.expr(\"try_cast(CRV as decimal(18,2))\").alias(\"crv\"),\n",
    "        F.expr(\"try_cast(DMC as decimal(18,2))\").alias(\"dmc\"),\n",
    "    )\n",
    "    .dropDuplicates([\"building_id\"])\n",
    ")\n",
    "\n",
    "# Hash to detect changes (only on descriptive attributes)\n",
    "src_building = src_building.withColumn(\n",
    "    \"attr_hash\",\n",
    "    F.sha2(\n",
    "        F.concat_ws(\"||\",\n",
    "            F.coalesce(F.col(\"university_id\").cast(\"string\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"country\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"state_province\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"building_name\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"building_size\").cast(\"string\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"building_type\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"built_year\").cast(\"string\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"fci\").cast(\"string\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"crv\").cast(\"string\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"dmc\").cast(\"string\"), F.lit(\"\")),\n",
    "        ),\n",
    "        256\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add SCD2 columns for insert rows\n",
    "src_building_ins = (\n",
    "    src_building\n",
    "    .withColumn(\"Start_Date\", load_dt)\n",
    "    .withColumn(\"End_Date\", F.lit(None).cast(\"date\"))\n",
    "    .withColumn(\"Active_Flag\", F.lit(True))\n",
    "    .withColumn(\"Load_Date\", load_ts)\n",
    "    .withColumn(\"Last_Updated_Date\", load_ts)\n",
    "    .withColumn(\"building_sk\", F.xxhash64(\"building_id\", F.col(\"Start_Date\").cast(\"string\")))\n",
    ")\n",
    "\n",
    "def scd2_merge(src_df, target_table, natural_key_col, sk_col):\n",
    "    \"\"\"\n",
    "    Generic SCD2 MERGE:\n",
    "    - matches on natural key AND Active_Flag = true\n",
    "    - if hash differs: close current record\n",
    "    - always insert new when not matched\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(target_table):\n",
    "        (\n",
    "            src_df.write.format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(target_table)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    tgt = DeltaTable.forName(spark, target_table)\n",
    "\n",
    "    # Close changed active rows\n",
    "    (\n",
    "        tgt.alias(\"t\")\n",
    "        .merge(\n",
    "            src_df.alias(\"s\"),\n",
    "            f\"t.{natural_key_col} = s.{natural_key_col} AND t.Active_Flag = true\"\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"t.attr_hash <> s.attr_hash\",\n",
    "            set={\n",
    "                \"End_Date\": load_dt,\n",
    "                \"Active_Flag\": F.lit(False),\n",
    "                \"Last_Updated_Date\": load_ts\n",
    "            }\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "# Merge Building dim\n",
    "scd2_merge(src_building_ins, DIM_BUILDING, \"building_id\", \"building_sk\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Asset Dimension (SCD2) - System/SubSystem/Component\n",
    "# ------------------------------------------------------------\n",
    "src_asset = (\n",
    "    b.select(\n",
    "        F.col(\"SystemCode\").alias(\"system_code\"),\n",
    "        F.col(\"SystemDescription\").alias(\"system_description\"),\n",
    "        F.col(\"SubsystemCode\").alias(\"subsystem_code\"),\n",
    "        F.col(\"SubsystemDescription\").alias(\"subsystem_description\"),\n",
    "        F.col(\"DescriptiveCode\").alias(\"component_code\"),\n",
    "        F.col(\"ComponentDescription\").alias(\"component_description\"),\n",
    "    )\n",
    "    .dropDuplicates([\"system_code\", \"subsystem_code\", \"component_code\"])\n",
    ")\n",
    "\n",
    "src_asset = src_asset.withColumn(\n",
    "    \"asset_nk\",\n",
    "    F.concat_ws(\"::\",\n",
    "        F.coalesce(\"system_code\", F.lit(\"\")),\n",
    "        F.coalesce(\"subsystem_code\", F.lit(\"\")),\n",
    "        F.coalesce(\"component_code\", F.lit(\"\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "src_asset = src_asset.withColumn(\n",
    "    \"attr_hash\",\n",
    "    F.sha2(\n",
    "        F.concat_ws(\"||\",\n",
    "            F.coalesce(F.col(\"system_description\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"subsystem_description\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"component_description\"), F.lit(\"\")),\n",
    "        ),\n",
    "        256\n",
    "    )\n",
    ")\n",
    "\n",
    "src_asset_ins = (\n",
    "    src_asset\n",
    "    .withColumn(\"Start_Date\", load_dt)\n",
    "    .withColumn(\"End_Date\", F.lit(None).cast(\"date\"))\n",
    "    .withColumn(\"Active_Flag\", F.lit(True))\n",
    "    .withColumn(\"Load_Date\", load_ts)\n",
    "    .withColumn(\"Last_Updated_Date\", load_ts)\n",
    "    .withColumn(\"asset_sk\", F.xxhash64(\"asset_nk\", F.col(\"Start_Date\").cast(\"string\")))\n",
    ")\n",
    "\n",
    "scd2_merge(src_asset_ins, DIM_ASSET, \"asset_nk\", \"asset_sk\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Fact Work Orders\n",
    "# ------------------------------------------------------------\n",
    "dim_b = spark.table(DIM_BUILDING).filter(\"Active_Flag = true\").select(\"building_id\", \"building_sk\")\n",
    "dim_a = spark.table(DIM_ASSET).filter(\"Active_Flag = true\").select(\"asset_nk\", \"asset_sk\")\n",
    "\n",
    "fact = (\n",
    "    b.select(\n",
    "        F.col(\"WOID\").alias(\"wo_id\"),\n",
    "        F.col(\"WODescription\").alias(\"wo_description\"),\n",
    "        F.expr(\"try_cast(WOPriority as int)\").alias(\"wo_priority\"),\n",
    "        F.expr(\"try_cast(WOStartDate as date)\").alias(\"wo_start_date\"),\n",
    "        F.expr(\"try_cast(WOEndDate as date)\").alias(\"wo_end_date\"),\n",
    "        F.expr(\"try_cast(WODuration as int)\").alias(\"wo_duration_days\"),\n",
    "        F.col(\"PPM_UPM\").alias(\"maintenance_type\"),\n",
    "\n",
    "        F.col(\"LaborCost\").cast(\"decimal(18,2)\").alias(\"labor_cost\"),\n",
    "        F.col(\"MaterialCost\").cast(\"decimal(18,2)\").alias(\"material_cost\"),\n",
    "        F.col(\"OtherCost\").cast(\"decimal(18,2)\").alias(\"other_cost\"),\n",
    "        F.col(\"TotalCost\").cast(\"decimal(18,2)\").alias(\"total_cost\"),\n",
    "        F.col(\"LaborHours\").cast(\"double\").alias(\"labor_hours\"),\n",
    "\n",
    "        F.col(\"MinTemp_C\").cast(\"double\").alias(\"min_temp_c\"),\n",
    "        F.col(\"MaxTemp_C\").cast(\"double\").alias(\"max_temp_c\"),\n",
    "        F.col(\"AtmosphericPressure_hPa\").cast(\"double\").alias(\"atmospheric_pressure_hpa\"),\n",
    "        F.col(\"Humidity_pct\").cast(\"double\").alias(\"humidity_pct\"),\n",
    "        F.col(\"WindSpeed_mps\").cast(\"double\").alias(\"wind_speed_mps\"),\n",
    "        F.col(\"WindDegree\").cast(\"double\").alias(\"wind_degree\"),\n",
    "        F.col(\"Precipitation_mm\").cast(\"double\").alias(\"precipitation_mm\"),\n",
    "        F.col(\"Snow_mm\").cast(\"double\").alias(\"snow_mm\"),\n",
    "        F.col(\"Cloudness_pct\").cast(\"double\").alias(\"cloudness_pct\"),\n",
    "\n",
    "        F.col(\"BuildingID\").alias(\"building_id\"),\n",
    "        F.col(\"SystemCode\").alias(\"system_code\"),\n",
    "        F.col(\"SubsystemCode\").alias(\"subsystem_code\"),\n",
    "        F.col(\"DescriptiveCode\").alias(\"component_code\"),\n",
    "\n",
    "        F.col(\"_ingest_ts\").alias(\"bronze_ingest_ts\"),\n",
    "        F.col(\"_source_file\").alias(\"bronze_source_file\"),\n",
    "        F.col(\"_batch_id\").alias(\"bronze_batch_id\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fact = fact.withColumn(\n",
    "    \"asset_nk\",\n",
    "    F.concat_ws(\"::\",\n",
    "        F.coalesce(\"system_code\", F.lit(\"\")),\n",
    "        F.coalesce(\"subsystem_code\", F.lit(\"\")),\n",
    "        F.coalesce(\"component_code\", F.lit(\"\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join surrogate keys\n",
    "fact = (\n",
    "    fact\n",
    "    .join(dim_b, on=\"building_id\", how=\"left\")\n",
    "    .join(dim_a, on=\"asset_nk\", how=\"left\")\n",
    "    .withColumn(\"Load_Date\", load_ts)\n",
    ")\n",
    "\n",
    "# Deduplicate WOID (keep latest ingest if duplicates)\n",
    "w = (\n",
    "    F.window(F.col(\"bronze_ingest_ts\"), \"36500 days\")  # dummy window; we’ll use max ingest\n",
    ")\n",
    "# Simple approach: order by ingest timestamp\n",
    "from pyspark.sql.window import Window\n",
    "dedupe_w = Window.partitionBy(\"wo_id\").orderBy(F.col(\"bronze_ingest_ts\").desc())\n",
    "\n",
    "fact = (\n",
    "    fact\n",
    "    .withColumn(\"_rn\", F.row_number().over(dedupe_w))\n",
    "    .filter(F.col(\"_rn\") == 1)\n",
    "    .drop(\"_rn\")\n",
    ")\n",
    "\n",
    "(\n",
    "    fact.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(FACT_WO)\n",
    ")\n",
    "\n",
    "print(\"✅ Silver tables created:\")\n",
    "print(\" -\", DIM_BUILDING)\n",
    "print(\" -\", DIM_ASSET)\n",
    "print(\" -\", FACT_WO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e422c60c-f0cd-4897-8f71-2706ac7982e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM fmucd_capstone.silver.dim_building_scd2 LIMIT 5;\n",
    "SELECT * FROM fmucd_capstone.silver.dim_asset_scd2 LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a299ee9d-19c0-4570-94b9-d345b84e4306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT building_id, COUNT(*) AS active_rows\n",
    "FROM fmucd_capstone.silver.dim_building_scd2\n",
    "WHERE Active_Flag = true\n",
    "GROUP BY building_id\n",
    "HAVING COUNT(*) > 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7d04d4-3f94-4665-95ad-5e19493eb733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT asset_nk, COUNT(*) AS active_rows\n",
    "FROM fmucd_capstone.silver.dim_asset_scd2\n",
    "WHERE Active_Flag = true\n",
    "GROUP BY asset_nk\n",
    "HAVING COUNT(*) > 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208dd8c4-3a88-4368-9ff1-2360183ae539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS fact_rows\n",
    "FROM fmucd_capstone.silver.fact_work_orders;\n",
    "\n",
    "SELECT\n",
    "  SUM(CASE WHEN building_sk IS NULL THEN 1 ELSE 0 END) AS missing_building_sk,\n",
    "  SUM(CASE WHEN asset_sk IS NULL THEN 1 ELSE 0 END) AS missing_asset_sk\n",
    "FROM fmucd_capstone.silver.fact_work_orders;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fef59c-89aa-4ad0-8673-c238fe1d998e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  SUM(CASE WHEN wo_start_date IS NULL THEN 1 ELSE 0 END) AS null_start,\n",
    "  SUM(CASE WHEN wo_end_date IS NULL THEN 1 ELSE 0 END) AS null_end,\n",
    "  SUM(CASE WHEN wo_end_date < wo_start_date THEN 1 ELSE 0 END) AS end_before_start\n",
    "FROM fmucd_capstone.silver.fact_work_orders;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8493399133639853,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_cleanse.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
