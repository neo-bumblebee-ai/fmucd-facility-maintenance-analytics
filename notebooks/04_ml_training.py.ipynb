{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f62688-0516-46c2-85ad-005461143816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "SOURCE_TABLE = \"fmucd_capstone.gold.work_orders_enriched\"\n",
    "df = spark.table(SOURCE_TABLE)\n",
    "\n",
    "# Fill system_code so we never lose rows due to nulls\n",
    "df = df.withColumn(\"system_code_filled\", F.coalesce(F.col(\"system_code\"), F.lit(\"UNKNOWN\")))\n",
    "\n",
    "# Compute a durable duration column:\n",
    "# - use wo_duration_days if present\n",
    "# - else compute datediff(end, start)\n",
    "df = df.withColumn(\n",
    "    \"duration_days_model\",\n",
    "    F.coalesce(\n",
    "        F.col(\"wo_duration_days\").cast(\"double\"),\n",
    "        F.when(\n",
    "            F.col(\"wo_start_date\").isNotNull() & F.col(\"wo_end_date\").isNotNull(),\n",
    "            F.datediff(F.col(\"wo_end_date\"), F.col(\"wo_start_date\")).cast(\"double\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Keep only rows where duration is available (this should NOT be empty now)\n",
    "df = df.filter(F.col(\"duration_days_model\").isNotNull())\n",
    "\n",
    "# Priority filled (don’t drop nulls)\n",
    "df = df.withColumn(\"wo_priority_filled\", F.coalesce(F.col(\"wo_priority\"), F.lit(-1)))\n",
    "\n",
    "print(\"Rows after building duration_days_model:\", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46b0293-c99d-4a4f-b546-75cab1e2c5d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "thresholds = (\n",
    "    df.groupBy(\"system_code_filled\", \"wo_priority_filled\")\n",
    "      .agg(F.expr(\"percentile_approx(duration_days_model, 0.90)\").alias(\"p90_duration\"))\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.join(thresholds, on=[\"system_code_filled\", \"wo_priority_filled\"], how=\"left\")\n",
    "      .withColumn(\n",
    "          \"label_high_duration\",\n",
    "          F.when(F.col(\"duration_days_model\") >= F.col(\"p90_duration\"), 1.0).otherwise(0.0)\n",
    "      )\n",
    ")\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"train:\", train_df.count(), \"test:\", test_df.count())\n",
    "display(train_df.groupBy(\"label_high_duration\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006da0ef-146d-4c92-8e37-dd4ddf54ae09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "SOURCE_TABLE = \"fmucd_capstone.gold.work_orders_enriched\"\n",
    "df = spark.table(SOURCE_TABLE)\n",
    "\n",
    "# Fill system_code so nulls don't drop rows\n",
    "df = df.withColumn(\"system_code_filled\", F.coalesce(F.col(\"system_code\"), F.lit(\"UNKNOWN\")))\n",
    "\n",
    "# Priority filled so nulls don't drop rows\n",
    "df = df.withColumn(\"wo_priority_filled\", F.coalesce(F.col(\"wo_priority\"), F.lit(-1)))\n",
    "\n",
    "# Compute model duration (use duration if present else derive from dates)\n",
    "df = df.withColumn(\n",
    "    \"duration_days_model\",\n",
    "    F.coalesce(\n",
    "        F.col(\"wo_duration_days\").cast(\"double\"),\n",
    "        F.when(\n",
    "            F.col(\"wo_start_date\").isNotNull() & F.col(\"wo_end_date\").isNotNull(),\n",
    "            F.datediff(F.col(\"wo_end_date\"), F.col(\"wo_start_date\")).cast(\"double\")\n",
    "        )\n",
    "    )\n",
    ").filter(F.col(\"duration_days_model\").isNotNull())\n",
    "\n",
    "# Threshold per peer group\n",
    "thresholds = (\n",
    "    df.groupBy(\"system_code_filled\", \"wo_priority_filled\")\n",
    "      .agg(F.expr(\"percentile_approx(duration_days_model, 0.90)\").alias(\"p90_duration\"))\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.join(thresholds, on=[\"system_code_filled\", \"wo_priority_filled\"], how=\"left\")\n",
    "      .withColumn(\"label_high_duration\",\n",
    "                  F.when(F.col(\"duration_days_model\") >= F.col(\"p90_duration\"), 1.0).otherwise(0.0))\n",
    ")\n",
    "\n",
    "# Split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"train:\", train_df.count(), \"test:\", test_df.count())\n",
    "display(train_df.groupBy(\"label_high_duration\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5191625-5a1b-46a1-bee1-e6e742c3c8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Categoricals (include filled keys instead of raw)\n",
    "cat_cols = [\"maintenance_type\", \"system_code_filled\", \"building_type\", \"state_province\", \"wo_priority_filled\"]\n",
    "\n",
    "num_cols = [\n",
    "    \"labor_hours\",\n",
    "    \"labor_cost\", \"material_cost\", \"other_cost\", \"total_cost\",\n",
    "    \"building_size\", \"fci\", \"dmc\",\n",
    "    \"min_temp_c\", \"max_temp_c\", \"atmospheric_pressure_hpa\", \"humidity_pct\",\n",
    "    \"wind_speed_mps\", \"wind_degree\", \"precipitation_mm\", \"snow_mm\", \"cloudness_pct\"\n",
    "]\n",
    "\n",
    "# Clean categoricals\n",
    "for c in [\"maintenance_type\", \"building_type\", \"state_province\"]:\n",
    "    train_df = train_df.withColumn(c, F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), F.lit(\"UNKNOWN\")).otherwise(F.col(c)))\n",
    "    test_df  = test_df.withColumn(c,  F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), F.lit(\"UNKNOWN\")).otherwise(F.col(c)))\n",
    "\n",
    "# Priority filled must be string to behave like category\n",
    "train_df = train_df.withColumn(\"wo_priority_filled\", F.col(\"wo_priority_filled\").cast(\"string\"))\n",
    "test_df  = test_df.withColumn(\"wo_priority_filled\",  F.col(\"wo_priority_filled\").cast(\"string\"))\n",
    "\n",
    "# Cast numerics safely\n",
    "for c in num_cols:\n",
    "    train_df = train_df.withColumn(c, F.expr(f\"try_cast({c} as double)\"))\n",
    "    test_df  = test_df.withColumn(c,  F.expr(f\"try_cast({c} as double)\"))\n",
    "\n",
    "# Drop all-null numeric cols\n",
    "counts = train_df.select([F.sum(F.when(F.col(c).isNotNull(), 1).otherwise(0)).alias(c) for c in num_cols]).collect()[0].asDict()\n",
    "kept_num_cols = [c for c in num_cols if counts.get(c, 0) > 0]\n",
    "dropped_num_cols = [c for c in num_cols if c not in kept_num_cols]\n",
    "\n",
    "print(\"Kept numeric cols:\", kept_num_cols)\n",
    "print(\"Dropped all-null numeric cols:\", dropped_num_cols)\n",
    "\n",
    "# Fill nulls\n",
    "train_df = train_df.fillna({c: 0.0 for c in kept_num_cols})\n",
    "test_df  = test_df.fillna({c: 0.0 for c in kept_num_cols})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5dacffa-a0c7-46bb-ad39-e459610398f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import mlflow, os\n",
    "import mlflow.spark\n",
    "\n",
    "\n",
    "mlruns_path = \"/Volumes/workspace/sor/fmucd/mlruns\"\n",
    "\n",
    "MODEL_NAME = \"high_duration_risk_model\"\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}__idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}__idx\" for c in cat_cols],\n",
    "    outputCols=[f\"{c}__ohe\" for c in cat_cols],\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "assembler_inputs = kept_num_cols + [f\"{c}__ohe\" for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"label_high_duration\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=50,\n",
    "    regParam=0.05\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler, lr])\n",
    "\n",
    "with mlflow.start_run(run_name=\"fmucd_high_duration_risk_lr\") as run:\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    preds = model.transform(test_df)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label_high_duration\", metricName=\"areaUnderROC\")\n",
    "    auc = evaluator.evaluate(preds)\n",
    "\n",
    "    mlflow.log_metric(\"auc\", float(auc))\n",
    "    mlflow.log_param(\"label_definition\", \"P90 duration per (system_code_filled, wo_priority_filled)\")\n",
    "    mlflow.log_param(\"numeric_features_used\", \",\".join(kept_num_cols))\n",
    "    mlflow.log_param(\"numeric_features_dropped_all_null\", \",\".join(dropped_num_cols))\n",
    "    mlflow.log_param(\"categorical_features\", \",\".join(cat_cols))\n",
    "\n",
    "\n",
    "    mlflow.spark.log_model(model, artifact_path=MODEL_NAME,dfs_tmpdir=\"/Volumes/workspace/sor/fmucd/mlruns/tmp\", pip_requirements=[\"pyspark==4.0.0\", \"mlflow\"])\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    print(\"✅ Training complete\")\n",
    "    print(\"Run ID:\", run_id)\n",
    "    print(\"AUC:\", auc)\n",
    "    print(\"Model URI:\", f\"runs:/{run_id}/{MODEL_NAME}\")\n",
    "\n",
    "    dbutils.jobs.taskValues.set(key=\"run_id\", value=run_id)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_ml_training.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
