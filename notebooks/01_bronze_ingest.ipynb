{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92386d4d-08ae-4142-b357-254975895bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# 01 — Bronze: Ingest FMUCD (Single CSV in Volumes) into Delta\n",
    "# - Reads the CSV from Volumes\n",
    "# - Sanitizes column names so Delta accepts them\n",
    "# - Adds ingestion metadata\n",
    "# - Writes a repeatable (idempotent) Bronze Delta table\n",
    "\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "CATALOG = \"fmucd_capstone\"\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "BRONZE_TABLE = \"bronze_fmucd_raw\"\n",
    "\n",
    "SOURCE_PATH = \"/Volumes/workspace/sor/fmucd/Facility Management Unified Classification Database (FMUCD).csv\"\n",
    "FULL_TABLE_NAME = f\"{CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TABLE}\"\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "def sanitize_col_name(c: str) -> str:\n",
    "    \"\"\"\n",
    "    Delta has restrictions on column names. This function converts FMUCD headers\n",
    "    (with spaces, %, parentheses, °, etc.) into Delta-safe names while preserving meaning.\n",
    "    \"\"\"\n",
    "    c = c.strip()\n",
    "\n",
    "    replacements = {\n",
    "        \"State/Province\": \"State_Province\",\n",
    "        \"FCI (facility condition index)\": \"FCI\",\n",
    "        \"CRV (current replacement value)\": \"CRV\",\n",
    "        \"DMC (deferred maintenance cost)\": \"DMC\",\n",
    "        \"PPM/UPM\": \"PPM_UPM\",\n",
    "        \"MinTemp.(°C)\": \"MinTemp_C\",\n",
    "        \"MaxTemp.(°C)\": \"MaxTemp_C\",\n",
    "        \"Atmospheric pressure(hPa)\": \"AtmosphericPressure_hPa\",\n",
    "        \"Humidity(%)\": \"Humidity_pct\",\n",
    "        \"WindSpeed(m/s)\": \"WindSpeed_mps\",\n",
    "        \"Precipitation(mm)\": \"Precipitation_mm\",\n",
    "        \"Snow(mm)\": \"Snow_mm\",\n",
    "        \"Cloudness(%)\": \"Cloudness_pct\",\n",
    "    }\n",
    "    if c in replacements:\n",
    "        return replacements[c]\n",
    "\n",
    "    # General cleanup\n",
    "    c = c.replace(\"/\", \"_\")\n",
    "    c = re.sub(r\"[ ,;{}()\\n\\t=]\", \"_\", c)  # invalid chars -> _\n",
    "    c = re.sub(r\"[^0-9a-zA-Z_]\", \"\", c)   # remove other non-safe chars (like °)\n",
    "    c = re.sub(r\"_+\", \"_\", c)             # collapse repeats\n",
    "    c = c.strip(\"_\")\n",
    "\n",
    "    return c\n",
    "\n",
    "# -------------------------\n",
    "# CREATE NAMESPACES\n",
    "# -------------------------\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}\")\n",
    "\n",
    "# -------------------------\n",
    "# READ CSV (RAW)\n",
    "# -------------------------\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")  # Bronze = keep as string\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "    .csv(SOURCE_PATH)\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# PARSING CHECK: CORRUPT RECORDS\n",
    "# -------------------------\n",
    "if \"_corrupt_record\" in df_raw.columns:\n",
    "    corrupt_cnt = df_raw.filter(F.col(\"_corrupt_record\").isNotNull()).count()\n",
    "    if corrupt_cnt > 0:\n",
    "        print(f\"⚠️ Corrupt records detected: {corrupt_cnt}\")\n",
    "        display(df_raw.filter(F.col(\"_corrupt_record\").isNotNull()).select(\"_corrupt_record\").limit(10))\n",
    "    else:\n",
    "        df_raw = df_raw.drop(\"_corrupt_record\")\n",
    "\n",
    "# -------------------------\n",
    "# SANITIZE COLUMN NAMES FOR DELTA\n",
    "# -------------------------\n",
    "original_cols = df_raw.columns\n",
    "sanitized_cols = [sanitize_col_name(c) for c in original_cols]\n",
    "\n",
    "# Ensure uniqueness (in case two columns sanitize to same name)\n",
    "seen = {}\n",
    "final_cols = []\n",
    "for c in sanitized_cols:\n",
    "    if c not in seen:\n",
    "        seen[c] = 0\n",
    "        final_cols.append(c)\n",
    "    else:\n",
    "        seen[c] += 1\n",
    "        final_cols.append(f\"{c}_{seen[c]}\")\n",
    "\n",
    "# Rename columns\n",
    "df_sanitized = df_raw\n",
    "for old, new in zip(original_cols, final_cols):\n",
    "    if old != new:\n",
    "        df_sanitized = df_sanitized.withColumnRenamed(old, new)\n",
    "\n",
    "# -------------------------\n",
    "# ADD METADATA\n",
    "# -------------------------\n",
    "df_bronze = (\n",
    "    df_sanitized\n",
    "    .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "    .withColumn(\"_source_file\", F.lit(SOURCE_PATH))\n",
    "    .withColumn(\"_batch_id\", F.date_format(F.current_timestamp(), \"yyyyMMdd_HHmmss\"))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# WRITE TO DELTA (IDEMPOTENT)\n",
    "# -------------------------\n",
    "(\n",
    "    df_bronze.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")                 # repeatable loads while building\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(FULL_TABLE_NAME)\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# VERIFY\n",
    "# -------------------------\n",
    "row_count = df_bronze.count()\n",
    "col_count = len(df_bronze.columns)\n",
    "\n",
    "print(f\"✅ Bronze table created/updated: {FULL_TABLE_NAME}\")\n",
    "print(f\"Rows: {row_count}\")\n",
    "print(f\"Columns: {col_count}\")\n",
    "\n",
    "# Show a few rows\n",
    "display(df_bronze.limit(10))\n",
    "\n",
    "# Optional: print the mapping for your Silver layer\n",
    "print(\"\\nColumn mapping (raw → bronze):\")\n",
    "for old, new in zip(original_cols, final_cols):\n",
    "    if old != new:\n",
    "        print(f\"  {old}  →  {new}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
